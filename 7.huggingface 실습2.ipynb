{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpuXkJtm6m8S"
   },
   "source": [
    "# Hugging Face Pipeline 개요 및 활용\n",
    "\n",
    "## 1. Pipeline API 개념 및 활용 범위  \n",
    "Hugging Face의 `pipeline` API는 NLP 모델 활용을 간소화하고 추상화한 고수준 인터페이스입니다.  \n",
    "- **개념**: 복잡한 모델 로딩, 토큰화, 추론 과정을 한 번에 처리하는 간단한 함수 호출 형태의 API.  \n",
    "- **장점**: 모델 활용 시 필수적인 전처리(토큰화) → 모델 추론 → 후처리(결과 해석) 과정을 모두 내부적으로 처리하므로, 사용자는 모델 사용 로직을 간단히 유지할 수 있습니다.  \n",
    "- **활용 범위**: 감정 분석(Sentiment Analysis), 제로샷 분류(Zero-Shot Classification), 질의응답(Question Answering), 번역(Translation), 요약(Summarization), 키워드 추출 등 다양한 NLP 태스크에 쉽게 적용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCC_dbt56m6M"
   },
   "source": [
    "## 2. 다양한 Pipeline 사례  \n",
    "- **Sentiment Analysis**: 입력 문장의 감정(긍정, 부정 등)을 추론하는 파이프라인.  \n",
    "- **Zero-Shot Classification**: 미리 정의되지 않은 레이블에 대해 문장을 분류할 수 있는 파이프라인.  \n",
    "- **Question-Answering(QA)**: 문서(컨텍스트)와 질문을 바탕으로 해당하는 답을 추론하는 파이프라인.  \n",
    "- **Summarization**: 긴 문서를 요약문으로 압축하는 파이프라인.  \n",
    "- **Translation**: 하나의 언어로 된 문장을 다른 언어로 번역하는 파이프라인.\n",
    "\n",
    "여기서는 특히 **질의응답(Question Answering) Pipeline**에 초점을 맞춰 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5RsZIB_zpKb"
   },
   "source": [
    "## 3. 질의응답(QA) Pipeline 개요  \n",
    "**Question-Answering Pipeline**은 주어진 문서(컨텍스트)와 하나의 질문을 입력받고, 해당 문서 내에서 답을 추출하는 파이프라인입니다.  \n",
    "- 입력:  \n",
    "  - `context`: 질문에 대한 해답을 포함하고 있을 것으로 기대되는 텍스트 문서  \n",
    "  - `question`: 답을 얻고자 하는 특정 질문  \n",
    "- 출력:  \n",
    "  - 문서 내에서 추출한 정답 문자열과 해당 정답의 점수, 위치 정보 등을 제공합니다.\n",
    "\n",
    "## 4. QA Pipeline 사용법\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416,
     "referenced_widgets": [
      "2f995af1fdf24fdbaa844af4fe4c7000",
      "7e5af6845a87404d8fbff586c1c6bd1c",
      "3728a33d673741268a4a8af9f26f8252",
      "37a68544de8c4e9a9b8796f7ed29eb8d",
      "1a0fd324379a4eb39156340228f28fc0",
      "a58917252f204147877d1e9befc34a36",
      "b1166fa149e84b78a49c8311fa994902",
      "7f7603628d69497184dd67c6de397fe0",
      "269ed9d1ff244bd7a5b6ef56719789a3",
      "ed604c9de56140aa8085354b7c400f00",
      "a6d3b55e867340ee81334bce3afe0b82",
      "5072c82a6a7243cb84dac5463786b224",
      "835c94cde84d49ca85381c3578bb98e3",
      "0e64ae6a10fe4fec989ce543d6337e71",
      "bdc18985f22948508be05bf1a141b0a2",
      "33bf72ca81a14f5d9fe2541d41e21819",
      "a56d791002d84932a5953704a000b463",
      "edd02403788e43db9e6e604b09bc60c9",
      "69421a637f9d43708ab2048189da6e46",
      "62a13be30699427bbd22cd6b479eafeb",
      "e18b78486b2b4c0aa8831abae1f01f61",
      "a7e03b8c42844bed8f07c228509c2db7",
      "6f4efc3dddef4339ad98c53db01a4ea7",
      "6bc13c2a09db4aeead307c68cd3cf02a",
      "4e05300860a847b1a16d00484e245e96",
      "f93d3325785946719e1ad5aeb4b30e6c",
      "7ca54cd3d2e54629b06957e725f1110a",
      "bf7d4febb4224588977ef501e9716389",
      "600bbeb24a254ee7b588fd877866784d",
      "2dd2c4435b59436e8327c2febb1cb5e4",
      "9f59454087524d8ea659eb307e8ab666",
      "d49656d207db4d59be5659c562031425",
      "3ed70f8126d04a89bd7f5b05c55cf320",
      "2e3392a381c1403cbeb9451a15b742e8",
      "e5984e2642804b40bb07989794323e93",
      "a80080d64dcf450cb966f3eaba295926",
      "e87f7406298b417c9eb88639328d76ab",
      "9ae2a0eb4f6b4790baad201dede10824",
      "04d1205b42534c098e57536299623f07",
      "4f824f9171cd4cb4b490ecefd4016cab",
      "8f04fc9d28ce45d0a81fdc761677b6ff",
      "e2326ae3ee7e49a1bb1b8a9fd31d4227",
      "3b9e897a51e349348b7000201691145f",
      "4baa512acfba4d6aafefd4961a5d3cf6",
      "ae9051d423f34392b0613bac36ca95e1",
      "66ed7f23801d4521a6fe51bd7b25aa88",
      "83a4a47038e34d0faacb7945b7b02e9f",
      "6f28eb778b724f39833a2c01ac645216",
      "4ebd36fe9aab4abbacb1a31b6f1df521",
      "16012682e2b746dc8eed30bc55edad92",
      "c7b0c96956b84c3b823d5dd19b36cc3e",
      "e6f089f6fe9e49d5941281abb3829886",
      "6ef5e70cbbc6459d8ed1bbf75c3b9a6d",
      "d66ffb4d61524549a0b1087b89fd79b6",
      "bdaa35848d0a43d086c9d4d6c224aa3f"
     ]
    },
    "executionInfo": {
     "elapsed": 45406,
     "status": "ok",
     "timestamp": 1734414631267,
     "user": {
      "displayName": "김민수",
      "userId": "14499279899039145671"
     },
     "user_tz": -540
    },
    "id": "LAsUz6ijzk5Y",
    "outputId": "d5fc8222-5d14-460b-b484-f557545c3a49"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f995af1fdf24fdbaa844af4fe4c7000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5072c82a6a7243cb84dac5463786b224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f4efc3dddef4339ad98c53db01a4ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e3392a381c1403cbeb9451a15b742e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae9051d423f34392b0613bac36ca95e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 중인 모델: DistilBertForQuestionAnswering\n",
      "모델 이름 혹은 경로: distilbert/distilbert-base-cased-distilled-squad\n",
      "사용 중인 토크나이저: DistilBertTokenizerFast\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# 'question-answering' 파이프라인을 로드합니다. 여기서 모델과 토크나이저는\n",
    "# 사전에 학습된 QA 모델(기본값: 'distilbert-base-cased-distilled-squad')을 기본 사용.\n",
    "# 인자로 model, tokenizer, device 등을 직접 설정 가능\n",
    "qa_pipeline = pipeline('question-answering')\n",
    "\n",
    "# 파이프라인 객체에서 모델과 토크나이저 접근하기\n",
    "model = qa_pipeline.model\n",
    "tokenizer = qa_pipeline.tokenizer\n",
    "\n",
    "print(\"사용 중인 모델:\", model.__class__.__name__)\n",
    "print(\"모델 이름 혹은 경로:\", model.name_or_path)\n",
    "print(\"사용 중인 토크나이저:\", tokenizer.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 253,
     "status": "ok",
     "timestamp": 1734414631518,
     "user": {
      "displayName": "김민수",
      "userId": "14499279899039145671"
     },
     "user_tz": -540
    },
    "id": "H2-N5yRJ7K6i",
    "outputId": "4fdecb75-d111-47a4-c702-6acd27c49d99"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/question_answering.py:391: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.8067139387130737, 'start': 14, 'end': 17, 'answer': 'NLP'}\n"
     ]
    }
   ],
   "source": [
    "result = qa_pipeline({\n",
    "    'context': \"Hugging Face는 NLP 분야에서 매우 유명한 오픈소스 커뮤니티이자 기업이다.\",\n",
    "    'question': \"Hugging Face는 어떤 분야에서 유명한가?\"\n",
    "})\n",
    "print(result)\n",
    "# 결과 예시:\n",
    "# {'score(신뢰도)': 0.95, 'start(시작 인덱스)': 18, 'end(끝 인덱스)': 21, 'answer(정답 텍스트)': 'NLP'}\n",
    "# 위 예제에서, context와 question을 dict 형태로 전달하면 파이프라인이 내부적으로\n",
    "# 토큰화 → 모델추론 → 답변 추출을 진행한 뒤 answer를 반환합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LAB8h4In7d1k"
   },
   "source": [
    "### 파이프라인 생성 시 원하는 모델 지정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264,
     "referenced_widgets": [
      "944778b463ab46a0b5d6b8532af7b5c1",
      "2697f16ebddd42b8ad8d4a70fa48c13a",
      "74959109664a430a82d871ceba6db27c",
      "01fdabe9ce6247839d3faa82b54f1e29",
      "0ddb09a4f37949b09302642a9378faa6",
      "f70168e25edc4a6e85f112fce0a209c5",
      "206bbcb12f6b4f5799d8acbb8bef424c",
      "f13ae4bfd4d44654bd50f76191b53121",
      "90ad7bd79a3f4a1589aaf5ad9630cf7f",
      "a995b5f5cd504ff4814e37190f147ebf",
      "3b75c6bd23a44ee791167a9352a3a150",
      "531ab124c8dc4b40af7442b78dc3ac1f",
      "2ff09fe853d54fa484abeb9864e90ba1",
      "108d9abd930d4437bd72ea8bee9c906e",
      "9a509871ed6f4180ac5392dedf52c88e",
      "1bd85f3a4b544dc685fed90252b30b4d",
      "b610fa7d5a304345a17be8142298c584",
      "c705b0a71cc9477cbf887fb13c99d7c1",
      "d6ab102c156a4ef48f36e77f162b5251",
      "d12c918459fc4949b184edc3ed6795df",
      "fa6266b2cdd6451790cf9e821d4a40b0",
      "bbf9a111b75941fdbc5e8dc6d249060b",
      "72ff355380264721a96b53b32663f65d",
      "18ec203fae624883b3c246e32d62cc4d",
      "91053c1591894cc4b426ed6d90fc0521",
      "9982a1bc65f440aa8abfc1809699c812",
      "2fc0cc36f88742eaa69b75bbba1237a5",
      "244816439ef74a9c9ca828fa14d8a941",
      "27b7fddcdbbf4fd7b211f6ff7c98f153",
      "3c2b4f07f2084982aa940a067aab2097",
      "16e0a2f74aed42668feab156fe447be7",
      "788f7cdfbd8d4aa1ac5fc1dce84e9f15",
      "f412f54a4bc3413ca0ed0ad09ce22640",
      "8fb7438de7b84e26bc88c299a3ccf709",
      "a40d4a1571c449d89ddd2a1022d91e86",
      "7a94bc82ebe545d8996cae957e1aca02",
      "af60e8945e4e4952851cc65fc31516e7",
      "7ebeb838b8b54c5ab0375989ad1fc0d0",
      "857b471cf8d648d69aa756a07e95c373",
      "acd7bef237674e9cb53edca1b466d1ed",
      "d04c3e051b0b4609a89fdeb8d70c889a",
      "ace83eeea5ed4903b76c01d1ff8ebf51",
      "05bdf1eccd7d4c299543b66cc09487b6",
      "ca9a666990b74a948f8cd64f300fa448",
      "e36ae3df6ebc422f84f4afe39c7c8bd7",
      "af01d83566be4a1f89d2875a4e2d8c7a",
      "ccb411c2aac1401b86e4201ad94e3151",
      "d869ab046fb245b6b5383af22576eba9",
      "f9c3a24214d34445b7fa0b7b417b47ba",
      "2315bf1fe6834c889cc4c80987b9bc1c",
      "dad0fbc2a676476abc0c5433132f1603",
      "e39844a355704d72877d1fea5adc0bdc",
      "ae39a667b32a4679a39c6caaeb701d3e",
      "6d8b2c135a6843d9a73ddfe118f4c046",
      "a0e50bd5b87542f4b98620684251fe29",
      "1c557cc77d9e4f919999abcf944337f4",
      "f176823114ef4a839e6afe2fee03a4ba",
      "f54b8cac3da2490a968bdee3f4008f50",
      "10f202bc7d4245f28a0b1957c9f7499e",
      "6904f70d45584635ad4289961cf7cb74",
      "79ac26008d8a45c585a9221c8281c434",
      "ae3fe7f8cd864069883264869e0bd175",
      "94ce86d5a2f24ed485b837ede8576624",
      "5f638bbf97a547429195387af989d008",
      "a8b57eda58e249fca61337695a167325",
      "b9e6ae409ef944f68b14f7162c66f73a"
     ]
    },
    "executionInfo": {
     "elapsed": 5120,
     "status": "ok",
     "timestamp": 1734414636636,
     "user": {
      "displayName": "김민수",
      "userId": "14499279899039145671"
     },
     "user_tz": -540
    },
    "id": "OxQTYI927hTl",
    "outputId": "e9ffe902-18fd-47fe-9eda-f24772a29dba"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "944778b463ab46a0b5d6b8532af7b5c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "531ab124c8dc4b40af7442b78dc3ac1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/496M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72ff355380264721a96b53b32663f65d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/79.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fb7438de7b84e26bc88c299a3ccf709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e36ae3df6ebc422f84f4afe39c7c8bd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c557cc77d9e4f919999abcf944337f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 중인 모델: RobertaForQuestionAnswering\n",
      "모델 이름 혹은 경로: deepset/roberta-base-squad2\n"
     ]
    }
   ],
   "source": [
    "# 특정 모델 이름 지정(예: 'bert-base-uncased'를 QA 모델로 활용)\n",
    "# 단, \"question-answering\" 태스크는 SQuAD 등 QA용으로 파인튜닝된 모델을 쓰는 것이 일반적이므로\n",
    "# 기본 BERT 모델을 바로 쓰면 성능이 낮을 수 있음.\n",
    "# 여기서는 QA 태스크용으로 학습된 모델 예를 들어봄: 'deepset/roberta-base-squad2'\n",
    "qa_pipeline_custom = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"deepset/roberta-base-squad2\",\n",
    "    tokenizer=\"deepset/roberta-base-squad2\"\n",
    ")\n",
    "\n",
    "print(\"사용 중인 모델:\", qa_pipeline_custom.model.__class__.__name__)\n",
    "print(\"모델 이름 혹은 경로:\", qa_pipeline_custom.model.name_or_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aak0Gyba0IbZ"
   },
   "source": [
    "## 5. 파이프라인 동작 흐름 이해\n",
    "1. **문서 입력**: 사용자가 질의응답 대상이 되는 텍스트(컨텍스트)와 질문을 입력.\n",
    "2. **토큰화**: context와 question을 합쳐 모델이 이해할 수 있는 토큰 형태로 변환.\n",
    "3. **모델 추론**: 사전 학습된 QA 모델(BERT 기반 등)을 통해 question과 context를 함께 입력받아 answer span을 예측.\n",
    "4. **답변 추출**: 예측된 시작 토큰과 종료 토큰 위치를 기반으로 context 문서에서 해당 범위의 텍스트를 추출하여 최종 답변으로 제공."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4VEB7ETA0IY8"
   },
   "source": [
    "## 6. 간단한 질의응답 봇 파인튜닝 실습\n",
    "짧은 위키피디아 문서를 하나 준비합니다.\n",
    "예: 위키피디아의 \"Python(언어)\" 문서 일부 발췌.\n",
    "사용자로부터 질문을 입력받고, QA pipeline에 해당 질문과 문서를 넣어 모델의 답변을 확인합니다.\n",
    "문서를 바꾸거나 질문을 바꿔가며 모델 반응을 살펴보고, 정확하지 않은 답변 시나리오를 통해 모델의 한계점을 파악해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 311,
     "status": "ok",
     "timestamp": 1734414636944,
     "user": {
      "displayName": "김민수",
      "userId": "14499279899039145671"
     },
     "user_tz": -540
    },
    "id": "GF9Di_xq0mGR",
    "outputId": "9dac7817-ac66-4835-f693-bca589f334da"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: Python은 누가 발표했는가?\n",
      "답변: Guido van Rossum\n",
      "점수(정확도 추정치): 0.41587844491004944\n",
      "질문: Python은 언제 발표되었는가?\n",
      "답변: Guido van Rossum (정확도: 0.41587844491004944 )\n",
      "질문: Python은 어느 행성에서 유래했는가?\n",
      "답변: Guido van Rossum (정확도: 0.3983064591884613 )\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face에서 제공하는 Transformers 라이브러리를 임포트합니다.\n",
    "# Transformers: 사전 학습된 NLP 모델과 토크나이저 등 다양한 기능을 제공하는 라이브러리\n",
    "from transformers import pipeline\n",
    "\n",
    "# question-answering 파이프라인을 불러옵니다.\n",
    "# pipeline('question-answering')은 기본적으로 QA를 수행하기 위한 모델과 토크나이저를 자동으로 로드합니다.\n",
    "qa_pipeline = pipeline('question-answering')\n",
    "\n",
    "# 아래 context는 위키피디아의 Python 언어 항목 중 일부를 발췌한 예시입니다.\n",
    "# (출처: Wikipedia \"Python (programming language)\" 문서 일부, 가상의 예시)\n",
    "context = \"\"\"\n",
    "Python은 1991년 귀도 반 로섬(Guido van Rossum)이 발표한 인터프리터 방식의 프로그래밍 언어로,\n",
    "코드 가독성이 우수하고 다양한 프로그래밍 패러다임을 지원한다.\n",
    "\"\"\"\n",
    "\n",
    "# 함수 예제: 질의응답을 수행하는 함수\n",
    "# 입력: (context: str, question: str)\n",
    "# 출력: dict 형태로 {'answer': str, 'score': float, 'start': int, 'end': int} 형식의 답변 정보\n",
    "def answer_question(context, question):\n",
    "    # 파이프라인에 context와 question을 dict 형태로 전달\n",
    "    result = qa_pipeline({\n",
    "        'context': context,\n",
    "        'question': question\n",
    "    })\n",
    "    return result\n",
    "\n",
    "# 예시 질의응답 수행\n",
    "# question: \"Python은 누가 발표했는가?\"\n",
    "# 기대되는 답변: \"귀도 반 로섬(Guido van Rossum)\"\n",
    "question = \"Python은 누가 발표했는가?\"\n",
    "result = answer_question(context, question)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"질문:\", question)\n",
    "print(\"답변:\", result['answer'])\n",
    "print(\"점수(정확도 추정치):\", result['score'])\n",
    "\n",
    "# 다른 질문 시도\n",
    "# question: \"Python은 언제 발표되었는가?\"\n",
    "# 기대 답변: \"1991년\"\n",
    "question = \"Python은 언제 발표되었는가?\"\n",
    "result = answer_question(context, question)\n",
    "print(\"질문:\", question)\n",
    "print(\"답변:\", result['answer'], \"(정확도:\", result['score'], \")\")\n",
    "\n",
    "# 모델 한계점 관찰을 위한 엉뚱한 질문\n",
    "# context에 없는 정보 질문: \"Python은 어느 행성에서 유래했는가?\"\n",
    "# 이 경우 문맥에 없는 정보이므로 답변이 어색하거나 문맥과 무관하게 추출될 수 있음.\n",
    "question = \"Python은 어느 행성에서 유래했는가?\"\n",
    "result = answer_question(context, question)\n",
    "print(\"질문:\", question)\n",
    "print(\"답변:\", result['answer'], \"(정확도:\", result['score'], \")\")\n",
    "\n",
    "# 이처럼 없는 정보에 대한 질문이나 모호한 질문, 사전학습 데이터와 관련이 적언 언어에 대해 모델이 부정확한 답변을 내놓을 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdvYpOIe0DsW"
   },
   "source": [
    "## 7. 모델 한계점 및 에러 케이스\n",
    "문서 안에 없는 정보에 대한 질문을 하면, 엉뚱한 답변을 하거나 최적의 답변을 찾지 못할 수 있습니다.\n",
    "문장이 애매하거나 중의적이면 모델이 문맥 파악을 제대로 하지 못할 수 있습니다.\n",
    "전문용어, 코드 예시, 숫자 등 특정 타입의 정보에 대해 정확도가 떨어질 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "buN-ktKm60Rv"
   },
   "source": [
    "### 간단한 데이터로 파인튜닝(Fine-tuning)하기\n",
    "\n",
    "- Hugging Face Transformers는 파인튜닝을 위해 Trainer API를 제공합니다.\n",
    "- 다음은 SQuAD 같은 질의응답 데이터셋 일부를 이용해 파인튜닝하는 간단한 예시 흐름입니다.\n",
    "\n",
    "**파인튜닝 개념**\n",
    "- 이미 QA 태스크로 사전학습(fine-tuned)된 모델을 더 작은 새로운 데이터에 맞춰 추가 훈련하는 과정.\n",
    "- 예시: 특정 도메인(의료, 법률)에 특화된 데이터로 파인튜닝하여 해당 도메인 질문에 더 정확히 답하도록 함.\n",
    "\n",
    "**실습**: QA 모델 파인튜닝(요약된 흐름)\n",
    "데이터 준비\n",
    "- context, question, answers(정답 범위를 지정한 형태)로 구성된 JSON 또는 CSV 형태 데이터 준비.\n",
    "- SQuAD 포맷 예시:\n",
    "```{\n",
    "  \"data\": [\n",
    "    {\n",
    "      \"title\": \"Example Title\",\n",
    "      \"paragraphs\": [\n",
    "        {\n",
    "          \"context\": \"여기는 파인튜닝용 문서 컨텍스트입니다.\",\n",
    "          \"qas\": [\n",
    "            {\n",
    "              \"id\": \"1\",\n",
    "              \"question\": \"여기는 무엇을 위한 문서인가?\",\n",
    "              \"answers\": [\n",
    "                {\n",
    "                  \"text\": \"파인튜닝용 문서\",\n",
    "                  \"answer_start\": 4\n",
    "                }\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "noC1O3cg-5Vw"
   },
   "source": [
    "아래 코드는 설명을 위한 예시이며, 실제로는 다양한 설정(하이퍼파라미터, 데이터셋 경로, 모델 경로) 조정이 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cpwrWCnkJcF4"
   },
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434,
     "referenced_widgets": [
      "77b84912061c42dabdd1ad082b6bff5e",
      "a4a7f16072644d1881601bbcb2258148",
      "ae254beeaa544333a39943e5ab36a309",
      "8b3bdc89cf3e48c083b3a7ebdb29b544",
      "888f47812d7942e6a7a03ea139bb3e77",
      "ca2531b2ac05480facefceeb5dd6c9c7",
      "b4314446001a4d93b5d5f30eea8d99eb",
      "f17f4d923a004b52bba0ec0d99fbc189",
      "50543f9a19b946a4bfdabb8d92e884e5",
      "14f6f0dbc1f9491eb77839ddece22072",
      "370f5ee7a694466095af46390d76c612",
      "0e2ac99f802144aead1a54d56bb8f1e7",
      "59224b5d02a4493a9d5c1aee32335041",
      "85d36e999d9e405f9b9c6ea823ba3037",
      "72e663083b4146b091ad7ea65cc1ad1c",
      "eb6b856420fd46d9b0ca1e0437223775",
      "35d9ad7768fe450f885221cd1a6e7b71",
      "fd1419b06b964f98893386781c5fcb12",
      "d2efdcb382cc4726b84b659825155ade",
      "abaa275376bc4704bb973be17208bc34",
      "a699358920a74bb8811c8e539ed3e8ca",
      "7da11415768b4bb096f861e62b7c2c64",
      "c1785aaa4ef14426856898b7cf27b640",
      "46f934b7682346bfaf62a292db82bd98",
      "bd1bf340c2594f6c866adb648571773e",
      "9abf522e62054b14aef06f2460fbc106",
      "ebb6ae9d9ed048a9833a4a4cd33a3a60",
      "23caf1f7456a44d9a074f71537791675",
      "2e4beed6c44340fd8ba049b0792730c6",
      "28513127b35443d08ccd4f2dfafd3ccc",
      "ae46a209c3a7449c903e65e952ad7fca",
      "eaf291015635496192597c093114eb9b",
      "13937821e3ec48e19ed12d1d54ea7a5d",
      "d96cdfcbfe1640bd9c4fe6672ec72491",
      "c58ea0bf26244f0892f0ccc73c8bede3",
      "d321ef4d0df2428291d38e093c24fc2e",
      "a30110bf702640bca9fb1b4a2a6f6109",
      "023d3a4b115b44428417d8f0e08b6004",
      "fe50e993881546348be15796bde14c56",
      "a042d298414940048261ca9cf22baee1",
      "88dbab012a9e46c0b2e184aad9a392d9",
      "2af0be7c90ce4b25892d3b6033a11366",
      "def97e979445449fbae0d97de47579b0",
      "ad8a7e46aa9d457c84db576e1ef0d52e",
      "cd61eff83c96454984d338ded2fef306",
      "54c381ec014e476ea92cf72335a86757",
      "c0c05b4baaf94d8b832d9879eeeb9b5c",
      "20e652163df347aba5c5d5befd144ddc",
      "b939860ac9bc4ba49a9db35a38f52882",
      "75ebc69c055b4f6dbac0bd4e9951207e",
      "696fb3e488284cd5a962e9dd1ec21653",
      "18dc5593a8e24d65ace2239217d9a298",
      "ec87c8cf8cad4b61886ee11d7d6fc508",
      "8038291e2ff14f17bd46e4ea6a7b008e",
      "25870e63fd4f4b2dbed6cb5087b901f6",
      "2e00e44cfa2e41aab639dbbb4d025d92",
      "8e547e6f7132430db719f759e9aff8cb",
      "f63c411436364483abfdfa92dceb6d83",
      "9a8c6654819b49c3af51186d1965f528",
      "c02388a952ae492a8759f75f646bd10c",
      "cfb61d22e0bb4ad388e6c66f2fe87aaf",
      "c448ac34412240e99ee4a46e35b8190c",
      "8661dc894eab47268af239da46741e93",
      "247e5ea419a04a8dab6dcf6b658076ea",
      "5c35772bb1fa429588083ab298814a60",
      "a11598576af94323ab53dbb5e49c4c85",
      "a55ff8aa54e34137be611105a21474d3",
      "4bca3918ace945659379a05cf42fc4f5",
      "f3a0b71c061a42209bf119727e9e0d81",
      "f707f4c7cb6c4119bca09be136023eff",
      "a6b884aa992344aaa884d402b8337417",
      "b9c77b5de7cf488b884a5c169b1ef017",
      "0158a8406db446cfa3b61e7c0018d2ec",
      "fa45f1ec74cf4e8ba6a25dffc069ad86",
      "ff93ee676f5f4470ac3ff4e21531095e",
      "9230569bbdc54decb6cb4b966f21f284",
      "8a1a6f08516648c5b29c58e4ca796e67",
      "5f7ce392c1b64b828c5efeba8f100424",
      "2a06490ac7fa4c2f85350ff6e933f524",
      "49e9dc51e4df4ee8ac7aaca103708afe",
      "448684bf41e94cc187e0cbd9fa474863",
      "31f552ce3f2442829c66b1e9bf72ac51",
      "3360823659dd49f888017712fb26217c",
      "1c5a9706142a465c979d510d67fe3d61",
      "5bc2e42c8d8b415b96815fab65beb2df",
      "227883cf677c4eff916fa665385862ce",
      "fa3062223cea4e9da9884f2528dd5430",
      "d55cfe0cfc9f4d8d818c2c6c314333b4",
      "c3363797240647b0920dc43c9147fc51",
      "67f5526c0bef4bc994fd1e1c4a6483b6",
      "bbf8a168cd47459aa2a676e454f13d65",
      "a65d46ec6f4b48789e3ab57ce19bd298",
      "468a8695daf0405b9c2252e4b5aca2f0",
      "fcc9b4ac12714265b46e576e9a1da8b6",
      "c93194b88cd94f84b4234259deb8a284",
      "3609fd1faf3b4a82951efbda9ba8d70f",
      "173a35dd4571465cbe43ff300f9c560c",
      "ce405710a833409284e6e11c51c6ddea",
      "6821627266b34c00be54ae40489a8aa1"
     ]
    },
    "executionInfo": {
     "elapsed": 59407,
     "status": "ok",
     "timestamp": 1734418962800,
     "user": {
      "displayName": "김민수",
      "userId": "14499279899039145671"
     },
     "user_tz": -540
    },
    "id": "9w-zGNJp1V6B",
    "outputId": "a90bd1a0-aece-45df-bd67-6362150d117f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77b84912061c42dabdd1ad082b6bff5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/6.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e2ac99f802144aead1a54d56bb8f1e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/11.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1785aaa4ef14426856898b7cf27b640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96cdfcbfe1640bd9c4fe6672ec72491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/60407 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd61eff83c96454984d338ded2fef306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/5774 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e00e44cfa2e41aab639dbbb4d025d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a55ff8aa54e34137be611105a21474d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/591 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f7ce392c1b64b828c5efeba8f100424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/263k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3363797240647b0920dc43c9147fc51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# datasets: 다양한 NLP 데이터셋 로드 및 처리 라이브러리\n",
    "# transformers: Hugging Face Transformers로 모델, 토크나이저, Trainer 등 사용\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "\n",
    "# 1) 데이터 로드: KorQuAD (https://huggingface.co/datasets/KorQuAD/squad_kor_v1)\n",
    "# KorQuAD는 한국어 기계독해(MRC) 데이터셋으로, SQuAD 형식을 따릅니다.\n",
    "# 전체 데이터를 로드하면 용량이 크기 때문에 여기서는 'validation' split만 로드합니다.\n",
    "val_data = load_dataset('KorQuAD/squad_kor_v1', split='validation')\n",
    "\n",
    "# 2) 데이터 분할: validation 데이터를 8:2 비율로 train과 test로 나눕니다.\n",
    "# train_test_split 함수는 Dataset 객체를 나누어 DatasetDict 형태로 반환합니다.\n",
    "split_data = val_data.train_test_split(test_size=0.2)\n",
    "\n",
    "# 분할된 데이터셋을 각각 변수에 저장:\n",
    "train_dataset = split_data['train']  # 전체 데이터의 80%를 훈련용으로 사용.\n",
    "valid_dataset = split_data['test']   # 나머지 20%를 검증용으로 사용.\n",
    "\n",
    "# 3) KoELECTRA 기반 KorQuAD 모델의 토크나이저 로드\n",
    "# monologg/koelectra-base-v3-finetuned-korquad: KoELECTRA 모델을 KorQuAD 데이터셋으로 미세 조정한 모델입니다.\n",
    "# AutoTokenizer는 해당 모델의 토크나이저를 자동으로 불러옵니다.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"monologg/koelectra-base-v3-finetuned-korquad\")\n",
    "\n",
    "# 4) 전처리 함수 정의: SQuAD 형식에 맞는 데이터 전처리\n",
    "# 이 함수는 각 데이터셋 예제의 질문(question), 문맥(context), 정답(answers)을 토크나이징하고,\n",
    "# 모델이 학습할 수 있도록 start_positions와 end_positions를 계산합니다.\n",
    "\n",
    "def preprocess_function(data):\n",
    "    \"\"\"\n",
    "    데이터셋을 모델 입력 형태로 변환하는 함수입니다.\n",
    "    질문과 문맥을 토크나이즈하고 정답의 시작/끝 위치를 계산하여 추가합니다.\n",
    "    \"\"\"\n",
    "    tokenized = tokenizer(\n",
    "        data[\"question\"],        # 질문 텍스트\n",
    "        data[\"context\"],         # 문맥 텍스트\n",
    "        truncation=True,         # 최대 길이를 초과하면 문맥을 잘라냄\n",
    "        padding='max_length',    # 최대 길이에 맞게 패딩 추가\n",
    "        max_length=384,          # 최대 토큰 길이 (일반적으로 384 사용)\n",
    "        return_offsets_mapping=True  # 각 토큰의 원본 텍스트 내 시작/끝 위치를 매핑\n",
    "    )\n",
    "\n",
    "    # 4-2. 정답의 시작(start_positions)과 끝(end_positions) 인덱스를 저장할 리스트 생성\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    # 4-3. 각 예제에 대해 토큰화된 오프셋과 원본 정답 위치를 비교하여 토큰 인덱스 계산\n",
    "    for i, offsets in enumerate(tokenized[\"offset_mapping\"]):\n",
    "        # KorQuAD v1 데이터의 answers 구조:\n",
    "        # {\"text\": [정답 문자열], \"answer_start\": [정답의 시작 위치]}\n",
    "        answer_start = data[\"answers\"][i][\"answer_start\"][0]  # 정답 시작 위치 (문서 내 위치)\n",
    "        answer_text = data[\"answers\"][i][\"text\"][0]           # 정답 문자열\n",
    "        answer_end = answer_start + len(answer_text)          # 정답 끝 위치\n",
    "\n",
    "        # 각 토큰이 문맥에 해당하는지를 나타내는 sequence_ids 구하기:\n",
    "        # sequence_ids는 각 토큰이 question(0)인지 context(1)인지 구분합니다.\n",
    "        sequence_ids = tokenized.sequence_ids(i)  # 각 토큰의 sequence id (question/context 구분)\n",
    "        context_start = sequence_ids.index(1)  # 문맥(context)이 시작하는 첫 토큰의 인덱스\n",
    "        context_end = len(sequence_ids) - 1 - sequence_ids[::-1].index(1)  # 문맥이 끝나는 마지막 토큰의 인덱스\n",
    "\n",
    "        # 정답 시작/끝 위치를 포함하는 토큰의 인덱스 초기화\n",
    "        start_token_index = context_start\n",
    "        end_token_index = context_start\n",
    "\n",
    "        # 오프셋을 순회하며 정답 시작과 끝이 포함된 토큰 인덱스를 찾습니다.\n",
    "        for idx, (offset_start, offset_end) in enumerate(offsets):\n",
    "            # 정답 시작 위치가 현재 토큰의 범위에 포함되면 start_token_index 설정\n",
    "            if offset_start <= answer_start < offset_end:\n",
    "                start_token_index = idx\n",
    "            # 정답 끝 위치가 현재 토큰의 범위에 포함되면 end_token_index 설정\n",
    "            if offset_start < answer_end <= offset_end:\n",
    "                end_token_index = idx\n",
    "        # 정답의 시작과 끝 토큰 인덱스를 리스트에 추가\n",
    "        start_positions.append(start_token_index)\n",
    "        end_positions.append(end_token_index)\n",
    "\n",
    "    # 4-4. 모델 입력에 필요하지 않은 \"offset_mapping\" 키를 제거합니다.\n",
    "    tokenized.pop(\"offset_mapping\", None)\n",
    "    # 4-5. 모델 학습을 위해 start_positions와 end_positions를 추가합니다.\n",
    "    tokenized[\"start_positions\"] = start_positions\n",
    "    tokenized[\"end_positions\"] = end_positions\n",
    "\n",
    "    # 최종적으로 토크나이즈된 데이터를 반환합니다.\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "81db2d7591444b0baa43e9dda9ac1550",
      "f23a807f4b8d4842acb0be73bd01b2a6",
      "4d88ff6dcafe4c47857abe1aa8c819b5",
      "c27462b30329403f875d26891b18df5f",
      "2173c5cae8e04013a600ee1adf2d4c5b",
      "2c107a76279f4e28a11b5c1675f6e9fe",
      "d80a5d549d63497c9b74a63a86c3eb51",
      "1dd6a372a0fc418f9981d0b1c317a4ab",
      "e0b52d655ff44fddaa11b69caca9ea57",
      "d890bcd606304351b147c39f6e317338",
      "a84e35a0bbe64d259610af186ce55822",
      "23a7ed64c7044e1ebdebd34e29f8133b",
      "a3c6ceb228fd467a9fc01defc90f822d",
      "eab84c94a9634833b2072374b90d8718",
      "8f382f19fa5d4f1e8450acf2ef13de50",
      "37af67a2899b4affb01a7c78ff87442e",
      "4daeace28afb4ffcb559d0bd9e0d4633",
      "96e58936ff0044a4931ded89511d420f",
      "612914e5313e40fd8322c035c4da8a1d",
      "113f407e034d417ea1b4e6fbfa1a47fa",
      "a0e26b727ce24010b2bb8744ccd82360",
      "791ba8b79efc405ea17b7fa8ffe7227a"
     ]
    },
    "executionInfo": {
     "elapsed": 12039,
     "status": "ok",
     "timestamp": 1734414661572,
     "user": {
      "displayName": "김민수",
      "userId": "14499279899039145671"
     },
     "user_tz": -540
    },
    "id": "GsiBSIkizGwh",
    "outputId": "fdc56143-7995-48ef-83b6-f848fbaba0a2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81db2d7591444b0baa43e9dda9ac1550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4619 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a7ed64c7044e1ebdebd34e29f8133b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1155 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# map 함수를 통해 실제 토큰화 적용\n",
    "train_tokenized = train_dataset.map(preprocess_function, batched=True)\n",
    "valid_tokenized = valid_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1734414661572,
     "user": {
      "displayName": "김민수",
      "userId": "14499279899039145671"
     },
     "user_tz": -540
    },
    "id": "iaOQCQYqlxv_",
    "outputId": "960b9acb-b14f-40ec-94c2-11b24c527e4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5월 27일, 지난 4월 29일 다라 지역에서 반정부 시위에 참여했던 13살 소년 함자 알하티브 (Hamza Al-Khatib) 가 고문 흔적이 가득한 시체로 집에 돌아오자, 시리아인들과 국제 사회의 분노가 빗발쳤다. 5월 29일, 홈스 주의 탈비셰흐와 라스탄에서 바샤르 알아사드 대통령에 반대하는 시위가 벌어졌으며, 정부의 탱크와 중기관총을 동원한 유혈 진압으로 11명이 살해되고, 무수한 사람들이 부상을 입었다. 탈비셰흐와 라스탄 이외에, 다라, 바니야스, 탈칼라크에서도 반정부 시위에 대한 유혈 진압이 계속되었다. 5월 30일, 반정부 시위자들과 정부군 간의 첫 무장 충돌이 일어났으며 , 시리아의 집권당 바트당은 전국민대화위원회를 구성하였다.\n",
      "반정부 시위자들과 정부군 간의 첫 무장 충돌이 일어난 날짜는 언제인가?\n",
      "{'text': ['5월 30일'], 'answer_start': [291]}\n"
     ]
    }
   ],
   "source": [
    "# 결과 확인\n",
    "print(valid_tokenized[0]['context'])  # 원본 텍스트\n",
    "print(valid_tokenized[0]['question'])\n",
    "print(valid_tokenized[0]['answers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452,
     "referenced_widgets": [
      "46993f496fef41afa2252d039989e32a",
      "4fee00fa5e4a4f1db186c49cafc894e0",
      "0f32d73796ee4ecf96aec7f825b3e484",
      "1da1455e8cd74ef484e55922c066d081",
      "8a662bbbe615434da060836ee8ebc542",
      "aaeef30e91b8428ab6490f286b50531c",
      "e0fe27efcbdc4634bcfb31aa94d629d7",
      "1112390a1c474f0bb1a92f931ad88c48",
      "71324d2a40b4471fa42a71a9b226f193",
      "9912dcb311ad421fb5874e47746be309",
      "cf4199c333b8417aba8cfe563ea3a3c4"
     ]
    },
    "executionInfo": {
     "elapsed": 805691,
     "status": "ok",
     "timestamp": 1734415467261,
     "user": {
      "displayName": "김민수",
      "userId": "14499279899039145671"
     },
     "user_tz": -540
    },
    "id": "3C8YafjB_JHB",
    "outputId": "e7b27356-19cf-45ab-c586-6dc73d86ec47"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46993f496fef41afa2252d039989e32a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/449M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "<ipython-input-9-41efcc9e9c78>:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ··········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20241217_055251-7o0dvdtr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/minsukim/huggingface/runs/7o0dvdtr' target=\"_blank\">./qa_finetuned_model</a></strong> to <a href='https://wandb.ai/minsukim/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/minsukim/huggingface' target=\"_blank\">https://wandb.ai/minsukim/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/minsukim/huggingface/runs/7o0dvdtr' target=\"_blank\">https://wandb.ai/minsukim/huggingface/runs/7o0dvdtr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='290' max='290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [290/290 11:27, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=290, training_loss=0.41807542998215247, metrics={'train_runtime': 793.2282, 'train_samples_per_second': 11.646, 'train_steps_per_second': 0.366, 'total_flos': 1810394579045376.0, 'train_loss': 0.41807542998215247, 'epoch': 2.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 로드\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"monologg/koelectra-base-v3-finetuned-korquad\")\n",
    "\n",
    "# 트레이닝 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qa_finetuned_model\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=2,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "# Trainer 생성\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,  # 실제 key는 데이터 구조에 따라 'train'일 수도 있고 아니면 그냥 [0]에 접근해야 할 수도 있음.\n",
    "    eval_dataset=valid_tokenized,   # 마찬가지로 데이터 구조 확인 필요\n",
    "    tokenizer=AutoTokenizer.from_pretrained(\"monologg/koelectra-base-v3-finetuned-korquad\")\n",
    ")\n",
    "\n",
    "# 파인튜닝 실행\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWB7oRZm_Prc"
   },
   "source": [
    "파인튜닝 후 확인\n",
    "- 파인튜닝 종료 후 trainer.save_model() 호출로 모델 저장 가능.\n",
    "- 저장된 경로를 이용해 pipeline(\"question-answering\", model=\"./qa_finetuned_model\") 형태로 파이프라인 생성 가능."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9j8cmOXV_S_R"
   },
   "source": [
    "파인튜닝된 모델 로드 및 파이프라인 재생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1357,
     "status": "ok",
     "timestamp": 1734415859189,
     "user": {
      "displayName": "김민수",
      "userId": "14499279899039145671"
     },
     "user_tz": -540
    },
    "id": "Hp-2EPoM_Uw_",
    "outputId": "4410d9e6-571b-43d6-df69-43fc8aeed7f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "병렬 처리 능력과 장거리 의존성 포착 능력은\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 파인튜닝 완료 후 저장한 모델 경로 사용\n",
    "finetuned_qa_pipeline = pipeline(\"question-answering\", model=\"/content/qa_finetuned_model/checkpoint-290\")\n",
    "\n",
    "result = finetuned_qa_pipeline({\n",
    "    'context': \"\"\"Transformer 모델은 그 혁신적인 구조와 뛰어난 성능으로 현대 NLP의 중심이 되었습니다.\n",
    "     병렬 처리 능력과 장거리 의존성 포착 능력은 이전 모델들의 한계를 극복했으며,\n",
    "     이는 더 큰 모델과 더 다양한 응용으로 이어지고 있습니다.\n",
    "     앞으로도 Transformer는 AI 발전의 핵심 요소로 계속 진화할 것으로 예상됩니다.\"\"\",\n",
    "    'question': \"Transformer는 이전 모델의 어떤 한계를 극복했는가?\"\n",
    "})\n",
    "\n",
    "print(result['answer'])\n",
    "# 파인튜닝된 데이터셋에 맞춰 개선된 결과를 기대할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "up7wJpl-_ZqQ"
   },
   "source": [
    "### 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eNSmmb9D_V44"
   },
   "source": [
    "- 파이프라인 모델/토크나이저 확인: pipeline 객체의 .model, .tokenizer 속성을 통해 현재 사용 중인 모델 정보 확인 가능.\n",
    "- 원하는 모델 지정: pipeline 생성 시 model=\"모델이름\", tokenizer=\"토크나이저이름\" 파라미터로 임의의 모델/토크나이저 지정 가능.\n",
    "- 파인튜닝: Trainer API와 데이터셋을 활용하여 사전 학습된 모델을 특정 데이터셋에 맞춰 재학습할 수 있으며, 완료 후 해당 모델을 파이프라인으로 다시 로드하여 사용 가능.\n",
    "\n",
    "이로써, 파이프라인의 기본 활용부터 모델 지정, 파인튜닝까지 일련의 과정을 모두 살펴볼 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMqeitAipKNoOnCL12TTqT3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
