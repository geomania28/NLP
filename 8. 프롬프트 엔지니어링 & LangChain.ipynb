{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyML4ZZTAKZna68kOTGZHovu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 프롬프트 엔지니어링 & LangChain 튜토리얼"],"metadata":{"id":"TFNkcPsYf7oO"}},{"cell_type":"markdown","source":["## 1.프롬프트 엔지니어링 개요"],"metadata":{"id":"GKd50No5f_5O"}},{"cell_type":"markdown","source":["### 1.1 프롬프트 엔지니어링이란?\n","\n","**프롬프트 엔지니어링(Prompt Engineering)**은 대규모 언어 모델(LLM, Large Language Model)을 활용할 때, 원하는 결과를 얻기 위해 모델에게 제공하는 입력(프롬프트)을 설계하고 다듬는 기법을 의미합니다.  \n","주어진 명령(Instruction), 예시(Examples), 컨텍스트(Context) 등을 체계적으로 구성해 모델의 출력을 제어하고 품질을 개선하는데 도움을 줍니다.\n","\n","### 1.2 프롬프트 엔지니어링의 중요성\n","\n","- **정확도 향상**: 단순히 질문을 던지는 것보다, 적절한 맥락, 형식, 예시를 제공하면 더 정확하고 일관된 답변을 얻을 수 있습니다.  \n","  - 예시 : 머신러닝 모델을 추천해줘 -> 파이썬으로 쉽게 사용할 수 있는 회귀 예측에 적합하고 가벼우면서 성능이 뛰어난 머신러닝 모델을 추천해줘.\n","- **일관성 보장**: 동일한 문맥, 형식을 유지하면 모델 답변의 변동성을 줄이고 일관성 있는 결과를 확보할 수 있습니다.  \n","- **특정 스타일/형식 유도**: 예시 답변을 제공하거나 출력 형식을 명시함으로써 모델이 특정 형식의 응답을 하도록 유도할 수 있습니다.\n","  - 예시 : 출력은 렌더링되지 않은 마크다운 형식으로 제공해줘\n","\n","### 1.3 기본적인 프롬프트 구성 전략\n","\n","1. **명확한 명령어 제시(Instruction)**: 모델에게 무엇을 해야 하는지 명확히 알려주기.\n","2. **맥락 제공(Context)**: 관련된 정보나 배경 지식을 제공.\n","3. **예시 제시(Example)**: 원하는 출력 형태를 보여주는 예시를 포함.\n","4. **출력 형식 지정(Output Formatting)**: 모델이 답변을 특정 포맷(리스트, JSON, 테이블 등)으로 내도록 요구.\n","5. **부가 조건 부여(Constraints)**: 글자 수 제한, 특정 언어 사용, 톤/스타일 지정 등.\n","\n","### 1.4 예시\n","\n","**안 좋은 프롬프트:**  \n","```\"파이썬 코드 짜줘, 사용자 입력받고 처리하면 돼.\"```\n","\n","**문제점:**  \n","- 어떤 사용자 입력을 어떻게 처리해야 하는지 모름.  \n","- 구체적인 컨텍스트가 없어 모델의 출력을 예측하기 어려움.\n","\n","**개선된 프롬프트:**  \n","```\"다음 요구사항을 충족하는 파이썬 코드를 작성해주세요:\n","\n","맥락: 사용자로부터 정수 n을 입력받아, 1부터 n까지의 합을 구한 뒤 결과를 출력하는 프로그램이 필요합니다.\n","\n","예시: 입력: 5 출력: 15 (1+2+3+4+5)\n","\n","요구사항:\n","\n","표준 입력 함수 input()을 통해 정수 하나를 입력받는다.\n","입력받은 정수 n에 대해, 1부터 n까지의 합을 계산한다.\n","계산 결과를 print()로 출력한다.\n","예외 처리는 필요없으며, n은 항상 1 이상의 정수라고 가정한다.\n","Python 3 호환 가능 코드\n","출력 형식:\n","\n","\n","n = int(input().strip())\n","# 여기서 합 계산 후 결과 print\n","\n","이와 같은 형태로 코드만 작성해주세요.\"\n","```\n","위와 같이 개선된 프롬프트는 모델에게 구체적인 명령어(해야 할 일), 맥락(문제 정의), 예시(입력-출력 예시), 출력 형식(함수 형태, 코드 블록), 그리고 부가 조건(가정, 함수명, 라이브러리 사용)에 대한 지시사항을 명확히 전달하므로 더 예측 가능하고 원하는 형태의 답변을 이끌어낼 수 있습니다."],"metadata":{"id":"cnADVAf6fg1w"}},{"cell_type":"markdown","source":["## 2.LangChain 개요"],"metadata":{"id":"e9GrOANhf4hZ"}},{"cell_type":"markdown","source":["### 2.1 LangChain이란?\n","\n","**LangChain**은 LLM 기반 애플리케이션을 쉽게 구성하고 관리할 수 있는 파이썬 기반 프레임워크입니다.  \n","- **핵심 기능**:  \n","  - LLM을 비롯한 다양한 모델 및 툴(예: 데이터베이스 질의, API 호출)과 연동을 쉽게 함.  \n","  - 대화형 에이전트(Agents), 체인(Chains), 메모리(Memory) 관리 기능 제공.  \n","  - 프롬프트 템플릿 관리로 프롬프트 엔지니어링을 체계화.\n","\n","### 2.2 LangChain 주요 개념\n","\n","- **PromptTemplate**: 프롬프트를 관리하고 변수를 넣어 반복적으로 재사용할 수 있는 템플릿.\n","- **LLMChain**: 특정 LLM과 PromptTemplate을 연결하여, 텍스트 입력을 받으면 해당 템플릿을 통해 생성된 프롬프트를 LLM에 전달하고 결과를 반환.\n","- **Memory**: 대화형 애플리케이션에서 이전 대화 내용을 기억하는 기능.\n","- **Agent & Tools**: LLM이 외부 API나 데이터베이스, 계산기 등의 툴을 사용할 수 있도록 하는 추상화."],"metadata":{"id":"ntUXAYtQfwdz"}},{"cell_type":"markdown","source":["## 3.프롬프트 엔지니어링 + LangChain 실습 예제\n","\n","아래 예제에서는 다음을 다룹니다:\n","\n","1. LangChain 설치 및 기본 활용법.\n","2. PromptTemplate을 이용해 프롬프트 구조화.\n","3. LLMChain으로 LLM에 요청을 보내고 결과 받아보기.\n","4. Memory를 활용한 간단한 대화흐름 유지.\n","5. Agent를 통해 간단한 작업 시퀀스 수행."],"metadata":{"id":"2qnq-XDRfsE5"}},{"cell_type":"markdown","source":["### 3.1 사전 준비\n","\n","- Python 환경에서 `langchain`과 관련 라이브러리 설치:"],"metadata":{"id":"AUZulQEdfn3m"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_sGCBgV6fSAt"},"outputs":[],"source":["!pip install langchain langchain-community openai==0.27.0\n","!pip install chromadb tiktoken==0.3.1"]},{"cell_type":"markdown","source":["### 3.2 기본 PromptTemplate 예제\n","- 아래 코드는 PromptTemplate을 사용해 사용자 입력을 받아 예시 출력 형식을 만들고, LLMChain으로 LLM에 요청을 보내는 기본 예제입니다."],"metadata":{"id":"C8OhKZ3egGJZ"}},{"cell_type":"code","source":["import os\n","from langchain import PromptTemplate, LLMChain\n","from langchain.llms import OpenAI # OpenAI LLM에 접근하기 위한 래퍼 클래스 제공\n","\n","# 본인의 API KEY 등록\n","os.environ[\"OPENAI_API_KEY\"] = \"여기에 본인의 API-KEY를 넣어 주세요\"\n","\n","# OpenAI 모델 초기화\n","# temperature: 응답의 창의성을 제어하는 하이퍼파라미터(0 ~ 1, 낮을수록 보수적)\n","llm = OpenAI(temperature=0.7)\n","\n","# PromptTemplate: 프롬프트에 변수 placeholders를 두고, 런타임 시 값 대입\n","# 아래 템플릿: {product} 변수에 제품명을 넣고, 그 제품의 장점 3가지를 나열하도록 모델에 요청\n","template = \"\"\"\n","당신은 친절한 마케터입니다.\n","아래 제품에 대해 간단한 마케팅 포인트 3가지를 bullet point로 제시하세요.\n","\n","제품: {product}\n","\"\"\"\n","\n","prompt = PromptTemplate(\n","    input_variables=[\"product\"],\n","    template=template\n",")\n","\n","# LLMChain: PromptTemplate과 LLM을 연결\n","marketing_chain = LLMChain(llm=llm, prompt=prompt)\n","\n","# 예시 입력 값 설정\n","product_name = \"무선 청소기\"\n","\n","# Chain 실행\n","# 이 때 prompt 템플릿의 {product}에 \"무선 청소기\"가 삽입됨\n","response = marketing_chain.run(product=product_name)\n","\n","print(\"입력 제품명:\", product_name)\n","print(\"응답 결과:\\n\", response)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UhWgbl-RgHkA","executionInfo":{"status":"ok","timestamp":1734281283758,"user_tz":-540,"elapsed":6119,"user":{"displayName":"김민수","userId":"14499279899039145671"}},"outputId":"50dc655b-704f-4ef7-d96c-fcb82af3bb7d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-2-87e23c249a12>:10: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n","  llm = OpenAI(temperature=0.7)\n","<ipython-input-2-87e23c249a12>:27: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n","  marketing_chain = LLMChain(llm=llm, prompt=prompt)\n","<ipython-input-2-87e23c249a12>:34: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n","  response = marketing_chain.run(product=product_name)\n"]},{"output_type":"stream","name":"stdout","text":["입력 제품명: 무선 청소기\n","응답 결과:\n"," \n","- 무선이기 때문에 전원 코드에 대한 걱정 없이 어디서나 편리하게 사용할 수 있습니다.\n","- 강력한 성능으로 먼지와 이물질을 효과적으로 제거하여 청결한 공간을 유지할 수 있습니다.\n","- 다양한 청소모드를 제공하여 다양한 환경에서도 최적의 청소 효과를 얻을 수 있습니다.\n"]}]},{"cell_type":"markdown","source":["위 예제에서는 product 변수를 템플릿에 주입해 모델이 마케팅 포인트를 나열하게 합니다.\n","프롬프트 엔지니어링을 통해 모델에게 \"친절한 마케터\" 역할을 지정하고 bullet point 형식 요청을 명확히 제시했습니다.\n","\n"],"metadata":{"id":"00TLWsEIgLQ9"}},{"cell_type":"markdown","source":["### 3.3 Memory 활용 (간단 대화흐름 유지)\n","- Memory를 사용하면 이전 사용자 입력과 모델 응답을 기억해서, 장시간 대화에서 컨텍스트를 유지할 수 있습니다."],"metadata":{"id":"qyZWdwdK_sok"}},{"cell_type":"code","source":["from langchain.chains import ConversationChain\n","from langchain.memory import ConversationBufferMemory\n","\n","# ConversationChain: 대화를 위한 체인\n","# ConversationBufferMemory: 이전 발화들을 메모리 버퍼에 저장\n","conversation_chain = ConversationChain(\n","    llm=llm,\n","    memory=ConversationBufferMemory(),\n","    verbose=True  # 어떤 Prompt가 LLM에 전달되는지 로깅\n",")\n","\n","# 사용자와의 대화 시뮬레이션\n","user_input_1 = \"안녕, 오늘 기분이 어때?\"\n","response_1 = conversation_chain.run(user_input_1)\n","print(\"사용자:\", user_input_1)\n","print(\"모델:\", response_1)\n","\n","# 모델은 메모리에 현재 대화 상태를 저장함.\n","user_input_2 = \"내가 아까 뭐 물어봤지?\"\n","response_2 = conversation_chain.run(user_input_2)\n","print(\"사용자:\", user_input_2)\n","print(\"모델:\", response_2)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nYrpp9FugOPg","executionInfo":{"status":"ok","timestamp":1734281360113,"user_tz":-540,"elapsed":3352,"user":{"displayName":"김민수","userId":"14499279899039145671"}},"outputId":"5c7821d4-070e-4f4a-c1f8-26b10ed7e09f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-4-e7b7c7461713>:8: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n","  memory=ConversationBufferMemory(),\n","<ipython-input-4-e7b7c7461713>:6: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n","  conversation_chain = ConversationChain(\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","\n","Human: 안녕, 오늘 기분이 어때?\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","사용자: 안녕, 오늘 기분이 어때?\n","모델:  안녕하세요! 저는 오늘 정말 좋은 기분이에요. 오늘은 저의 프로그래밍이 업데이트 되었는데요, 그래서 더욱 더 많은 정보를 학습하고 여러분의 질문에 대답할 수 있게 되었어요. 여러분은 오늘 어떤 일이 있었나요?\n","\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","Human: 안녕, 오늘 기분이 어때?\n","AI:  안녕하세요! 저는 오늘 정말 좋은 기분이에요. 오늘은 저의 프로그래밍이 업데이트 되었는데요, 그래서 더욱 더 많은 정보를 학습하고 여러분의 질문에 대답할 수 있게 되었어요. 여러분은 오늘 어떤 일이 있었나요?\n","Human: 내가 아까 뭐 물어봤지?\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","사용자: 내가 아까 뭐 물어봤지?\n","모델:  아까 여러분이 물어보신 질문은 \"오늘 기분이 어때요?\"라는 질문이었어요. 제가 알기로는 오늘 여러분은 꽤 바쁜 하루를 보내셨던 것 같아요. 그래서 저도 여러분을 위해 최대한 많은 정보를 알아내려고 노력했어요. 하지만 제가 아직 모르는 것도 많아요. 무엇을 도와드릴까요?\n"]}]},{"cell_type":"markdown","source":["여기서 conversation_chain은 Memory를 통해 이전 대화(\"안녕, 오늘 기분이 어때?\"라는 질문)를 기억하고 \"내가 아까 뭐 물어봤지?\"라는 후속 질문에 정확히 반응할 수 있습니다."],"metadata":{"id":"HkDetf9pgQNY"}},{"cell_type":"markdown","source":["### 3.4 Agent와 Tools 활용\n","- LangChain 에이전트(Agent)는 LLM이 도구(tools, 예: 검색, 계산기, DB 질의)를 사용하도록 하여, 단순히 텍스트 응답 이상의 동작을 수행하게 합니다.\n","\n","- 아래 예제에서는 매우 단순한 Tool을 만들어 Agent가 실행하도록 하는 흐름을 간단히 보여줍니다."],"metadata":{"id":"nH8KGR1igSV9"}},{"cell_type":"code","source":["from langchain.agents import load_tools, initialize_agent, AgentType\n","from langchain.tools import BaseTool\n","import ast\n","\n","# 간단한 Tool 구현\n","# 입력한 숫자 리스트의 합을 계산하는 툴\n","class SumTool(BaseTool):\n","    name: str = \"sum_tool\"\n","    description: str = \"숫자 리스트를 받아 합계를 반환한다.\"\n","\n","    def _run(self, numbers: str) -> str:\n","        # numbers 예: \"[10, 20, 30]\" 형태로 들어올 수 있음\n","        # ast.literal_eval을 사용해 문자열을 실제 리스트로 변환\n","        num_list = [int(x.strip()) for x in numbers.split(\",\")]\n","        return str(sum(num_list))\n","\n","# 툴 인스턴스 생성\n","sum_tool = SumTool()\n","\n","# 에이전트 초기화\n","# 'zero-shot-react-description' 에이전트 타입: LLM에 도구 사용 방법을 자동 추론하게 함.\n","agent = initialize_agent(\n","    tools=[sum_tool],\n","    llm=llm,\n","    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n","    verbose=True\n",")\n","\n","# 에이전트에게 특정 작업 지시\n","# 예: \"다음 숫자들의 합을 구해줘: 10, 20, 30\"\n","user_query = \"다음 숫자들의 합을 구해줘: 10, 20, 30\"\n","response = agent.run(user_query)\n","\n","print(\"사용자 질의:\", user_query)\n","print(\"에이전트 응답:\", response)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BH9aVxTCgaFJ","executionInfo":{"status":"ok","timestamp":1734282893959,"user_tz":-540,"elapsed":1696,"user":{"displayName":"김민수","userId":"14499279899039145671"}},"outputId":"158f47af-2ed4-4bab-9f79-296145818ec5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m I should use the sum_tool to calculate the sum of the given numbers.\n","Action: sum_tool\n","Action Input: 10, 20, 30\u001b[0m\n","Observation: \u001b[36;1m\u001b[1;3m60\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3m Now I know the final answer is 60.\n","Final Answer: 60\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","사용자 질의: 다음 숫자들의 합을 구해줘: 10, 20, 30\n","에이전트 응답: 60\n"]}]},{"cell_type":"markdown","source":["### 3.5 LAG (Retrieval-Augmented Generation) 실습 예제\n","\n","- **LAG(Retrieval-Augmented Generation)**은 LLM이 가진 \"고정된 지식\"의 한계를 극복하기 위한 방법입니다. 모델을 호출하기 전에 사용자 질의와 관련된 문서(파일, 데이터베이스 등)를 검색하여 해당 정보를 프롬프트에 추가하면, 모델이 최신 정보나 특정 도메인 지식을 활용해 더 풍부하고 정확한 답변을 생성할 수 있습니다.\n","\n","#### 핵심 아이디어:\n","\n","- **문제점**: LLM은 사전학습 이후의 최신 정보나 특정 도메인 지식을 자체 파라미터만으로 활용하기 어렵습니다.\n","- **해결책**: 모델 호출 전 관련 문서를 검색(Retrieve)하고 이를 프롬프트에 포함시켜(LAG) 모델이 해당 지식을 바탕으로 답을 생성하도록 합니다.\n","\n","- **장점**:\n","  - 모델 파라미터 변경 없이도 외부 데이터로 지식을 확장 가능.\n","  - 특정 분야 문서(사내 자료, 전문 분야 매뉴얼 등) 활용으로 전문성 강화\n","  - 최신 정보 반영 및 사실성(Factuality) 개선"],"metadata":{"id":"ENKmrNjbBeyR"}},{"cell_type":"markdown","source":["아래 실습 예제는 간단한 텍스트 파일을 벡터화한 뒤 검색해 LLM에 전달하고, 이를 기반으로 정확한 답변을 생성하는 흐름을 보여줍니다.\n","\n","**주요 절차:**\n","1. **문서 로드**: 로컬 텍스트 파일을 불러와 LangChain의 Document 형태로 관리.\n","2. **벡터화(Vectorization) 및 인덱싱**: 문서를 임베딩하여 벡터 스토어(예: Chroma, FAISS)에 저장.\n","3. **질의 응답**: 사용자가 질문하면, 관련 문서 조각을 검색(Retrieve)하여 LLM에 함께 전달.\n","4. **모델 응답**: LLM은 제공된 문서 정보를 바탕으로 답변 생성.\n","\n","**사전 준비**\n","\n","- 로컬 텍스트 파일(`data.txt`) 준비. 예: `data.txt` 내용:"],"metadata":{"id":"AW0FFc0JBzX-"}},{"cell_type":"code","source":["from langchain.document_loaders import TextLoader # langchain.document_loaders: 로컬/원격 문서 로딩\n","from langchain.embeddings.openai import OpenAIEmbeddings # langchain.embeddings: 텍스트를 벡터로 변환하는 임베딩 모델 제공\n","from langchain.vectorstores import Chroma # langchain.vectorstores: 임베딩한 벡터를 검색 가능한 스토어에 저장/검색\n","from langchain.llms import OpenAI\n","from langchain.chains import RetrievalQA # langchain.chains import RetrievalQA: Retrieve-then-Read 형태 QA 체인\n","import os\n","\n","# 1) 문서 로드\n","# TextLoader: 로컬 텍스트 파일을 Document 형태로 로딩\n","loader = TextLoader(\"/content/data.txt\", encoding=\"utf-8\")\n","documents = loader.load()\n","\n","# documents 구조 예시: [Document(page_content=\"파일 내용 문자열\", metadata={})]\n","\n","# 2) 벡터화 및 인덱싱\n","# OpenAIEmbeddings: 텍스트를 벡터로 변환하기 위해 OpenAI 임베딩 모델 사용\n","embeddings = OpenAIEmbeddings()\n","\n","# Chroma: in-memory 벡터DB를 사용해 유사도 검색 수행\n","vectorstore = Chroma.from_documents(documents, embeddings)\n","\n","# 3) Retriever 설정\n","# vectorstore.as_retriever()를 통해 검색 인터페이스 생성\n","retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n","# k=2는 관련 문서 2개까지 가져오겠다는 의미\n","\n","# 4) RetrievalQA 체인 생성\n","# LLM: OpenAI, retrieval: 위에서 만든 retriever\n","# combine_documents_chain: 검색된 문서를 LLM 프롬프트에 포함해 답변 생성\n","qa_chain = RetrievalQA.from_chain_type(\n","  llm=OpenAI(temperature=0.7),\n","  chain_type=\"stuff\",  # 단순히 검색 문서를 연결하여 전달하는 체인 타입\n","  retriever=retriever\n",")\n","\n","# 질의응답 예시\n","question = \"파이썬은 누가 언제 발표했지?\"\n","answer = qa_chain.run(question)\n","\n","print(\"질문:\", question)\n","print(\"답변:\", answer)\n","\n","# 여기서 LLM은 검색된 문서 내용(파이썬 발표자 및 연도 정보)을 참조하여 답변을 생성.\n"],"metadata":{"id":"uTJ1qte8CWjc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["결과 해석\n","\n","질문: \"파이썬은 누가 언제 발표했지?\"\n","LLM은 data.txt의 내용을 벡터 검색을 통해 참조하고, \"귀도 반 로섬이 1991년에 발표\"했다는 정답을 제시할 수 있음.\n","\n","이로써, Retrieval-Augmented Generation(LAG) 기법을 이용하여 단순 대화형 에이전트나 체인에서 한 단계 나아가, 외부 지식을 효과적으로 활용하는 방법을 살펴보았습니다."],"metadata":{"id":"qYlUOmjMCbLp"}},{"cell_type":"markdown","source":["## 4.정리 및 응용 아이디어\n","**프롬프트 엔지니어링:** 모델에게 역할(Role), 형식(Format), 제약(Constraints)를 명확히 전달해 원하는 답변 품질을 높일 수 있습니다.\n","\n","**LangChain:**\n"," - PromptTemplate을 통해 반복적 패턴의 프롬프트 관리 용이.\n"," - LLMChain으로 프롬프트와 모델 연결 간소화.\n"," - Memory로 대화 컨텍스트 유지.\n"," - Agents/Tools를 통해 LLM의 활용 범위 확대(질의응답, 검색, 계산 등).\n","\n","**응용 아이디어:**\n"," - 특정 도메인(의학, 법률, 기술) 컨텍스트를 풍부하게 제공하는 프롬프트를 만들어 정확도 향상.\n"," - 사용자 입력을 바탕으로 커스터마이징 가능한 PromptTemplate 생성(예: 장문의 문서 요약, 스타일 변환 등).\n"," - 외부 데이터베이스와 연동해 LLM을 \"지식 에이전트\"로 활용.\n"," - 여러 PromptTemplate과 체인을 연결해 복잡한 작업 흐름을 자동화."],"metadata":{"id":"48bRWcYwgcKv"}}]}